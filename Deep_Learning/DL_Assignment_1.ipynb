{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DL Assignment 1",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE6qcw_j8Pi2"
      },
      "source": [
        "In this homework assignment, you are requested to implement a full backprop algorithm using only *numpy*.\n",
        "\n",
        "- We assume sigmoid activation across all layers.\n",
        "- We assume a single value in the output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UV4RvXYL8k85"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRml6glFIPCa"
      },
      "source": [
        "The following class represents a simple feed forward network with multiple layers. The network class provides methods for running forward and backward for a single instance, throught the network. You should implement the methods (indicated with TODO), that performs forward and backward for an entire batch. Note, the idea is to use matrix multiplications, and not running standard loops over the instances in the batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLdNoCt58qg5"
      },
      "source": [
        "class MyNN:\n",
        "  def __init__(self, learning_rate, layer_sizes):\n",
        "    '''\n",
        "    learning_rate - the learning to use in backward\n",
        "    layer_sizes - a list of numbers, each number repreents the nuber of neurons\n",
        "                  to have in every layer. Therfore, the length of the list \n",
        "                  represents the number layers this network has.\n",
        "    '''\n",
        "    self.learning_rate = learning_rate\n",
        "    self.layer_sizes = layer_sizes\n",
        "    self.model_params = {}\n",
        "    self.memory = {}\n",
        "    self.grads = {}\n",
        "    \n",
        "    # Initializing weights\n",
        "    for layer_index in range(len(layer_sizes) - 1):\n",
        "      W_input = layer_sizes[layer_index + 1]\n",
        "      W_output = layer_sizes[layer_index]\n",
        "      self.model_params['W_' + str(layer_index + 1)] = np.random.randn(W_input, W_output) * 0.1\n",
        "      self.model_params['b_' + str(layer_index + 1)] = np.random.randn(W_input) * 0.1\n",
        "      \n",
        "      \n",
        "  def forward_single_instance(self, x):    \n",
        "    a_i_1 = x\n",
        "    self.memory['a_0'] = x\n",
        "    for layer_index in range(len(self.layer_sizes) - 1):\n",
        "      W_i = self.model_params['W_' + str(layer_index + 1)]\n",
        "      b_i = self.model_params['b_' + str(layer_index + 1)]\n",
        "      z_i = np.dot(W_i, a_i_1) + b_i\n",
        "      a_i = 1/(1+np.exp(-z_i))\n",
        "      self.memory['a_' + str(layer_index + 1)] = a_i\n",
        "      a_i_1 = a_i\n",
        "    return a_i_1\n",
        "  \n",
        "  \n",
        "  def log_loss(self, y_hat, y):\n",
        "    '''\n",
        "    Logistic loss, assuming a single value in y_hat and y.\n",
        "    '''\n",
        "    m = y_hat[0]\n",
        "    cost = -y[0]*np.log(y_hat[0]) - (1 - y[0])*np.log(1 - y_hat[0])\n",
        "    return cost\n",
        "  \n",
        "  \n",
        "  def backward_single_instance(self, y):\n",
        "    a_output = self.memory['a_' + str(len(self.layer_sizes) - 1)]\n",
        "    dz = a_output - y\n",
        "     \n",
        "    for layer_index in range(len(self.layer_sizes) - 1, 0, -1):\n",
        "      print(layer_index)\n",
        "      a_l_1 = self.memory['a_' + str(layer_index - 1)]\n",
        "      dW = np.dot(dz.reshape(-1, 1), a_l_1.reshape(1, -1))\n",
        "      self.grads['dW_' + str(layer_index)] = dW\n",
        "      W_l = self.model_params['W_' + str(layer_index)]\n",
        "      dz = (a_l_1 * (1 - a_l_1)).reshape(-1, 1) * np.dot(W_l.T, dz.reshape(-1, 1))\n",
        "      # TODO: calculate and memorize db as well.\n",
        "  \n",
        "  # TODO: update weights with grads\n",
        "  #def update(self): \n",
        "  \n",
        "  # TODO: implement forward for a batch X.shape = (network_input_size, number_of_instance)\n",
        "  #def forward_batch(self, X)\n",
        "  \n",
        "  # TODO: implement backward for a batch y.shape = (1, number_of_instance)\n",
        "  #def backward_batch(self, y)\n",
        "  \n",
        "  # TODO: implement log_loss_batch, for a batch of instances\n",
        "  # def log_loss(self, y_hat, y):"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qib6W4QXO644"
      },
      "source": [
        "nn = MyNN(0.01, [3, 2, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nQR8QllPf_5"
      },
      "source": [
        "nn.model_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXiyn-yrPC6-"
      },
      "source": [
        "x = np.random.randn(3)\n",
        "y = np.random.randn(1)\n",
        "\n",
        "y_hat = nn.forward_single_instance(x)\n",
        "print(y_hat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5M50i3plclj"
      },
      "source": [
        "nn.backward_single_instance(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWnZB1YmYnIt"
      },
      "source": [
        "def train(X, y, epochs, batch_size):\n",
        "  '''\n",
        "  Train procedure, please note the TODOs inside\n",
        "  '''\n",
        "  for e in range(1, epochs + 1):\n",
        "    epoch_loss = 0\n",
        "    # TODO: shuffle\n",
        "    batches = #... TODO: divide to batches\n",
        "    for X_b, y_b in batches:\n",
        "      y_hat = nn.forward_batch(X_b)\n",
        "      epoch_loss += nn.log_loss_batch(y_hat, y_b)\n",
        "      nn.backward_batch(y_b)\n",
        "      nn.update()\n",
        "    print(f'Epoch {e}, loss={epoch_loss/len(batches)}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE1ydWlatkty"
      },
      "source": [
        "# TODO: Make sure the following network trains properly\n",
        "\n",
        "nn = MyNN(0.001, [6, 4, 3, 1])\n",
        "\n",
        "X = np.random.randn(6, 100)\n",
        "y = np.random.randn(1, 100)\n",
        "batch_size = 8\n",
        "epochs = 2\n",
        "\n",
        "train(X, y, epochs, batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dY4scUksulC"
      },
      "source": [
        "#TODO: train on an external dataset\n",
        "\n",
        "Train on the Bike Sharing dataset, using the same split as in *DL Notebook 4 - logistic regression*.\n",
        "Use the following features from the data:\n",
        "\n",
        "* temp\n",
        "* atemp\n",
        "* hum\n",
        "* windspeed\n",
        "* weekday\n",
        "\n",
        "The response variable is, like in Notebook 4, raw[\"success\"] = raw[\"cnt\"] > (raw[\"cnt\"].describe()[\"mean\"]).\n",
        "\n",
        "The architecture of the network should be: [5, 40, 30, 10, 7, 5, 3, 1].\n",
        "\n",
        "Use batch_size=8, and train it for 100 epochs on the train set (based on the split as requested above).\n",
        "\n",
        "Then, plot loss per epoch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Y0GvPArZear"
      },
      "source": [
        "# TODO: your code goes here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}